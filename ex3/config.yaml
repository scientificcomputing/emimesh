cluster:
  export CUDA_VISIBLE_DEVICES=$SLURM_JOB_GPUS &&
  echo $CUDA_VISIBLE_DEVICES &&
  mkdir -p logs/{rule} &&
  sbatch
    --partition={resources.partition}
    --ntasks={resources.ntasks}
    --nodes={resources.nodes}
    --mem={resources.mem_mb}
    --job-name={rule}-{wildcards}
    --output=logs/{rule}/{wildcards}-%j.out
    --time={resources.time}
    --gres=gpu:{resources.gpus}
default-resources:
  - partition=defq,fpgaq,dgx2q,hgx2q #,milanq
  - mem_mb=0
  - time="60:00"
  - ntasks=1
  - nodes=1
  - gpus=0
restart-times: 0
max-jobs-per-second: 20
max-status-checks-per-second: 5
local-cores: 20
latency-wait: 30
jobs: 500
keep-going: True
rerun-incomplete: True
printshellcmds: True
scheduler: greedy
use-conda: True
rerun-triggers: mtime